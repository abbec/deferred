% Deferred Shading
% Albert Cervin

# Introduction
Deferred shading is an old idea that was proposed by CITE in
1988. However, it has not been used extensively until recent
years. Today, deferred shading is de facto standard in game engines
and also in other types of real-time rendering. The need for deferred
shading arises in scenes with many dynamic light sources. In these
cases deferred shading can simplify and speed up lighting calculations
by order of magnitude. There are downsides however and the memory
bandwidth requirement of the algorithm is high, even with modern
graphics cards.

Deferred shading is also known under names such as deferred rendering
and deferred lighting. However, all three names essentially describes
the same algorithm.

# Method

## The G-buffer
Before any rendering happends, the scene contains geometric
information. In the classic rendering approach, called forward
rendering, each object is rendered and for each object, lighting
calculations are performed. The problem with this arises when there
are many dynamic light sources. The most common technique for this is
to use what is called über-shaders. Über shaders means that the
shaders are compiled in different permutations, one for each number of
dynamic light sources. This arises from the fact that it is not
possible to use a non-constant value in a shader loop. This kind of
loop is needed to sum all contributions from all lights for an
object. The problem with über-shaders are that the number of
permutations does not scale well. If the number of light sources is
increased, the number of shader permutations is increased
accordingly. This is not ideal.

Deferred shading tackles this problem by deferring the lighting
calculations to a later stage in the algorithm. This makes deferred
shading a multi-pass algorithm and means that the hardware has to
support multiple render targets (often referred to as MRT). The
multiple render targets are used for, in the first pass of the
algorithm, storing geometric scene information. The geometric
information is typically at least view space normals and depth.

This buffer is called a G-buffer and this stage of the algorithm is
called geometry stage. Vertex normals are transformed into view-space
by multiplying with world and view matrix.

EQUATION

To have sufficient precision in the normals for later calculations it
is possible to use 16-bit textures to store the normals. This is
however both wasteful and inefficient on modern hardware. The normals
can therefore be transformed into spheremap coordinates and then
stored in an 8-bit texture. CITE (check aras_p)

Depth can also be stored in the G-buffer. It is however possible to
use the already existing depth buffer generation and bind the depth
buffer as a shader resource in later stages. One important thing with
the depth buffer generated by hardware is that it contains projected
depth values, that is, clip space depth. To use this depth in
calculations it has to be unprojected by dividing it with the distance
to the far clip plane. 

The depth can furthermore be used to reconstruct view space positions
which means that the positions does not need to be stored in the G-buffer.

## Implementation
The deferred shading algoritm has been implemented in the DirectX
SDK. The minimum required DirectX version is 10 which means that a
computer running at least Windows Vista is required to run the sample.

The implementation uses 16 bit textures (render targets) to achieve the maximum visual
quality. The normals X, Y and Z values are stored in R, G and B
channels of one render target, respectively.

# Results
The result is an implementation of a deferred shading algorithm in the
DirectX SDK. The implementation runs in real-time and rendering
statistics for the algorithm are presented below.

# Discussion
